[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/aalanov/laith/REPA/generate.py", line 215, in <module>
[rank0]:     main(args)
[rank0]:   File "/home/aalanov/laith/REPA/generate.py", line 64, in main
[rank0]:     model = SiT_models[args.model](
[rank0]:   File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1343, in to
[rank0]:     return self._apply(convert)
[rank0]:   File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/nn/modules/module.py", line 903, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/nn/modules/module.py", line 903, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:   File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1329, in convert
[rank0]:     return t.to(
[rank0]: RuntimeError: CUDA error: out of memory
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank0]:[W119 01:16:08.568337953 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0119 01:16:09.138758 19282 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 19315) of binary: /home/aalanov/.conda/envs/repa/bin/python3.9
Traceback (most recent call last):
  File "/home/aalanov/.conda/envs/repa/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/aalanov/.conda/envs/repa/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
generate.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-19_01:16:09
  host      : cn-043.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 19315)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
